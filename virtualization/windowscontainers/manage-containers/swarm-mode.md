---
title: Начало работы с режимом мелких объектов
description: Инициализация кластера мелких объектов, создание сети наложения и привязка службы к сети.
keywords: docker, контейнеры, управление группой мелких объектов
author: kallie-b
ms.date: 02/9/2017
ms.topic: article
ms.prod: windows-containers
ms.service: windows-containers
ms.assetid: 5ceb9626-7c48-4d42-81f8-9c936595ad85
ms.openlocfilehash: 560e9ffc92728628268d7d557b8fa8428316c8ec
ms.sourcegitcommit: 1ca9d7562a877c47f227f1a8e6583cb024909749
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 12/04/2019
ms.locfileid: "74909684"
---
# <a name="getting-started-with-swarm-mode"></a>Начало работы с режимом мелких объектов 

## <a name="what-is-swarm-mode"></a>Что такое "режим мелких объектов"?
Режим мелких объектов — это функция Docker, которая предоставляет встроенные возможности управление контейнерами, в том числе кластеризацию узлов Docker и планирование рабочих нагрузок контейнера. Группа узлов Docker формирует кластер «мелких объектов», когда их модули Docker работают вместе в «режиме мелких объектов». Дополнительные сведения о режиме мелких объектов см. в [на основном сайте документации Docker](https://docs.docker.com/engine/swarm/).

## <a name="manager-nodes-and-worker-nodes"></a>Управляющие узлы и рабочие узлы
Группа мелких объектов состоит из двух типов узлов контейнера: *управляющих узлов* и *рабочих узлов*. Каждая группа мелких объектов инициализируется через управляющий узел, при этом все команды интерфейса командной строки Docker для мониторинга группы мелких объектов и управления ей должны выполняться с одного из управляющих узлов. Управляющие узлы можно рассматривать как «хранителей» состояния группы мелких объектов — вместе они формируют группу согласия, которая предоставляет сведения о состоянии служб, запущенных в группе мелких объектов. Их задача — убедиться, что фактическое состояние группы мелких объектов всегда соответствует желаемому состоянию, заданному разработчиком или администратором. 

>[!NOTE]
>Любой заданный Swarm может иметь несколько узлов диспетчера, но всегда должен иметь *хотя бы один*узел. 

Рабочие узлы управляются группой мелких объектов Docker посредством управляющих узлов. Чтобы присоединиться к группе мелких объектов, рабочий узел должен использовать «маркер присоединения», созданный с управляющего узла при инициализации группы мелких объектов. Рабочие узлы просто получают и выполняют задачи от управляющих узлов, поэтому для них не требуется информация о состоянии группы мелких объектов.

## <a name="swarm-mode-system-requirements"></a>Системные требования для режима мелких объектов

По крайней мере одна физическая или виртуальная система компьютера (для использования полной функциональности Swarm по крайней мере на двух узлах) рекомендуется использовать либо **Windows 10 Creators Update** , либо **Windows Server 2016** *со всеми последними обновлениями\** установка в качестве узла контейнера (см. раздел [контейнеры Windows в Windows 10](https://docs.microsoft.com/virtualization/windowscontainers/quick-start/quick-start-windows-10) или [контейнеры Windows в Windows Server](https://docs.microsoft.com/virtualization/windowscontainers/quick-start/quick-start-windows-server) ) для получения дополнительных сведений о начале работы с контейнерами DOCKER в Windows 10.

\***Примечание**. для DOCKER Swarm в Windows Server 2016 требуется [KB4015217](https://support.microsoft.com/help/4015217/windows-10-update-kb4015217)

**Подсистема DOCKER v 1.13.0 или более поздняя**

Открытые порты: следующие порты должны быть доступны на каждом узле. В некоторых системах эти порты по умолчанию открыты.
- TCP-порт 2377 для управления кластером
- Порт TCP и UDP 7946 для связи между узлами
- Порт UDP 4789 для трафика сети наложения

## <a name="initializing-a-swarm-cluster"></a>Инициализация кластера мелких объектов

Чтобы инициализировать группу мелких объектов, просто выполните следующую команду на одном из узлов контейнера (заменив \<HOSTIPADDRESS\> на локальный IPv4-адрес хост-компьютера):

```
# Initialize a swarm 
C:\> docker swarm init --advertise-addr=<HOSTIPADDRESS> --listen-addr <HOSTIPADDRESS>:2377
```
После выполнения этой команды на узле контейнера модуль Docker на нем начинает работать в режиме мелких объектов в качестве управляющего узла.

## <a name="adding-nodes-to-a-swarm"></a>Добавление узлов в группу мелких объектов

Для использования функций режима Swarm и перекрытия сети *не* требуется несколько узлов. Все функции группы мелких объектов и сети наложения доступны при наличии одного узла, работающего в режиме мелких объектов (т. е. управляющего узла, переведенного в режим мелких объектов с помощью команды `docker swarm init`).

### <a name="adding-workers-to-a-swarm"></a>Добавление рабочих узлов в группу мелких объектов

После инициализации группы мелких объектов управляющим узлом в нее можно добавить рабочие узлы с помощью другой простой команды:

```
C:\> docker swarm join --token <WORKERJOINTOKEN> <MANAGERIPADDRESS>
```

Здесь \<MANAGERIPADDRESS\> — это локальный IP-адрес управляющего узла группы мелких объектов, а \<WORKERJOINTOKEN\> — это маркер присоединения рабочего узла из выходных данных команды `docker swarm init`, которая была выполнена на управляющем узле. Маркер присоединения также можно получить, выполнив одну из следующих команд на управляющем узле после инициализации группы мелких объектов:

```
# Get the full command required to join a worker node to the swarm
C:\> docker swarm join-token worker

# Get only the join-token needed to join a worker node to the swarm
C:\> docker swarm join-token worker -q
```

### <a name="adding-managers-to-a-swarm"></a>Добавление управляющих узлов в группу мелких объектов
Дополнительные управляющие узлы можно добавить в кластер мелких объектов с помощью следующей команды:

```
C:\> docker swarm join --token <MANAGERJOINTOKEN> <MANAGERIPADDRESS>
```

Опять же, \<MANAGERIPADDRESS\> — это локальный IP-адрес управляющего группы мелких объектов. Маркер присоединения, \<MANAGERJOINTOKEN\> — это маркер присоединения *управляющего узла* для группы мелких объектов, который можно получить, выполнив одну из следующих команд на существующем управляющем узле:

```
# Get the full command required to join a **manager** node to the swarm
C:\> docker swarm join-token manager

# Get only the join-token needed to join a **manager** node to the swarm
C:\> docker swarm join-token manager -q
```

## <a name="creating-an-overlay-network"></a>Создание сети наложения

После настройки кластера мелких объектов можно создать сети наложения. Для этого выполните следующую команду на управляющем узле группы мелких объектов:

```
# Create an overlay network 
C:\> docker network create --driver=overlay <NETWORKNAME>
```

Здесь \<NETWORKNAME\> — это имя вашей сети.

## <a name="deploying-services-to-a-swarm"></a>Развертывание служб в группе мелких объектов
После создания сети наложения можно создать службы и привязать их к сети. Служба создается с помощью следующего синтаксиса:

```
# Deploy a service to the swarm
C:\> docker service create --name=<SERVICENAME> --endpoint-mode dnsrr --network=<NETWORKNAME> <CONTAINERIMAGE> [COMMAND] [ARGS…]
```

Здесь \<SERVICENAME\> — это имя службы, которое будет использоваться для ссылки на службу при обнаружении служб (для чего применяется собственный DNS-сервер Docker). \<NETWORKNAME\> — это имя сети, к которой вы хотите подключить эту службу (например, "Мйоверлайнет"). \<КОНТАИНЕРИМАЖЕ\> — имя образа контейнера, который будет определять службу.

>[!NOTE]
>Второй аргумент для этой команды, `--endpoint-mode dnsrr`, необходим для указания подсистемы DOCKER, которая будет использоваться политикой циклического перебора DNS для балансировки сетевого трафика между конечными точками контейнера службы. В настоящее время циклический перебор DNS является единственной стратегией балансировки нагрузки, поддерживаемой в Windows Server 2016. [Сетка маршрутизации](https://docs.docker.com/engine/swarm/ingress/) для узлов Windows DOCKER поддерживается в windows Server 2019 (и более поздних версиях), но не в windows Server 2016. Пользователи, которые ищут альтернативную стратегию балансировки нагрузки в Windows Server 2016, уже сегодня могут настроить внешний балансировщик нагрузки (например, NGINX) и использовать [режим публикации порта](https://docs.docker.com/engine/reference/commandline/service_create/#/publish-service-ports-externally-to-the-swarm--p---publish) Swarm, чтобы предоставить порты узла контейнера, по которым будет распределяться трафик.

## <a name="scaling-a-service"></a>Масштабирование службы
После развертывания службы в кластере мелких объектов экземпляры контейнера, из которых она состоит, развертываются в кластере. По умолчанию количество экземпляров контейнера, поддерживающих службу (число «реплик» или «задач» для службы) равно одному. Тем не менее службу можно создать с несколькими задачами, используя параметр `--replicas` для команды `docker service create` или масштабируя службу после ее создания.

Масштабируемость службы — важное преимущество Docker Swarm, и им можно воспользоваться с помощью одной команды Docker:

```
C:\> docker service scale <SERVICENAME>=<REPLICAS>
```

Здесь \<SERVICENAME\> — имя масштабируемой службы, а \<REPLICAS\> — это число задач или экземпляров контейнера, до которого масштабируется служба.


## <a name="viewing-the-swarm-state"></a>Просмотр состояния группы мелких объектов

Существует несколько полезных команд для просмотра состояния группы мелких объектов и служб, запущенных в ней.

### <a name="list-swarm-nodes"></a>Перечисление узлов группы мелких объектов
Выполните следующую команду, чтобы просмотреть список узлов, присоединенных к группе мелких объектов, включая сведения о состоянии каждого узла. Эту команда необходимо выполнить на **управляющем узле**.

```
C:\> docker node ls
```

В выходных данных этой команды можно заметить, что один из узлов помечен звездочкой (*). Этот символ просто обозначает текущий узел, на котором была выполнена команда `docker node ls`.

### <a name="list-networks"></a>Перечисление сетей
Выполните следующую команду, чтобы просмотреть список сетей на заданном узле. Для просмотра сетей наложения эту команда следует выполнить на **управляющем узле**, работающем в режиме мелких объектов.

```
C:\> docker network ls
```

### <a name="list-services"></a>Перечисление служб
Выполните следующую команду, чтобы просмотреть список служб, запущенных в группе мелких объектов, а также сведения об их состоянии.

```
C:\> docker service ls
```

### <a name="list-the-container-instances-that-define-a-service"></a>Перечисление экземпляров контейнера, которые определяют службу
Выполните следующую команду, чтобы просмотреть сведения об экземплярах контейнера, запущенных для указанной службы. В выходные данные этой команды включены идентификаторы и узлы, на которых работает каждый контейнер, а также информация о состоянии контейнеров.  

```
C:\> docker service ps <SERVICENAME>
```
## <a name="linuxwindows-mixed-os-clusters"></a>Кластеры со смешанными ОС Linux и Windows

Недавно член нашей команды опубликовал короткое руководство из трех частей, рассказывающее о том, как с помощью режима мелких объектов Docker настроить приложение для смешанных ОС Windows + Linux. Если вы только начинаете работать с режимом мелких объектов Docker или использовать его для запуска приложений для смешанных ОС, это руководство будет очень полезным. Ознакомьтесь с ним.
- [Использование DOCKER Swarm для запуска контейнерного приложения Windows + Linux (часть 1/3)](https://www.youtube.com/watch?v=ZfMV5JmkWCY&t=170s)
- [Использование DOCKER Swarm для запуска контейнерного приложения Windows + Linux (часть 2/3)](https://www.youtube.com/watch?v=VbzwKbcC_Mg&t=406s)
- [Использование DOCKER Swarm для запуска контейнерного приложения Windows + Linux (часть 3/3)](https://www.youtube.com/watch?v=I9oDD78E_1E&t=354s)

### <a name="initializing-a-linuxwindows-mixed-os-cluster"></a>Инициализация кластера смешанных ОС Linux + Windows
Инициализировать кластер мелких объектов смешанных ОС не составит труда, если правила вашего брандмауэра корректно настроены и у ваших узлов есть доступ друг к другу. Для добавления узла Linux в группу мелких объектов потребуется лишь стандартная команда `docker swarm join`:
```
C:\> docker swarm join --token <JOINTOKEN> <MANAGERIPADDRESS>
```
Из узла Linux группу мелких объектов можно также инициализировать с помощью той же команды, которая используется для инициализации группы мелких объектов из узла Windows:
```
# Initialize a swarm 
C:\> docker swarm init --advertise-addr=<HOSTIPADDRESS> --listen-addr <HOSTIPADDRESS>:2377
```

### <a name="adding-labels-to-swarm-nodes"></a>Добавление меток в узлы группы мелких объектов
Чтобы запустить службу Docker в кластере мелких объектов смешанных ОС, должен быть способ определить, какие узлы группы мелких объектов работают под управлением ОС, для которой предназначена эта служба, а какие нет. [Метки объектов Docker](https://docs.docker.com/engine/userguide/labels-custom-metadata/) — это удобный способ помечать узлы таким образом, чтобы службы можно было создавать и настраивать для выполнения только на тех узлах, которые соответствуют их ОС. 

>[!NOTE]
>[Метки объектов DOCKER](https://docs.docker.com/engine/userguide/labels-custom-metadata/) можно использовать для применения метаданных к различным объектам DOCKER (включая образы контейнеров, контейнеры, тома и сети), а также в различных целях (например, метки могут использоваться для разделения компонентов внешнего интерфейса и серверной части приложения, разрешая интерфейсные микрослужбы сечедулед только на узлах с внешними метками и серверной миркосервицес, запланированными только на внутренних узлах с метками). В этом случае метки используются для того, чтобы узлы ОС Windows можно было отличить от узлов ОС Linux.

Чтобы пометить существующие узлы группы мелких объектов, используйте следующий синтаксис:

```
C:\> docker node update --label-add <LABELNAME>=<LABELVALUE> <NODENAME>
```

Здесь `<LABELNAME>` — имя создаваемой метки. В этом примере мы проводим различие между узлами по их ОС, поэтому логичным именем для этой метки может быть "os". `<LABELVALUE>` является значением метки. в этом случае можно выбрать использование значений "Windows" и "Linux". Разумеется, вы можете присваивать собственные имена и значения меткам, однако они не должны противоречить друг другу. `<NODENAME>` — имя узла, на который вы добавляете метки; Вы можете напомнить себе имена узлов, запустив `docker node ls`. 

**Например**, если у вас есть четыре узла группы мелких объектов в кластере (то есть два узла Windows и два узла Linux), команды для обновления метки могут иметь следующий вид:

```
# Example -- labeling 2 Windows nodes and 2 Linux nodes in a cluster...
C:\> docker node update --label-add os=windows Windows-SwarmMaster
C:\> docker node update --label-add os=windows Windows-SwarmWorker1
C:\> docker node update --label-add os=linux Linux-SwarmNode1
C:\> docker node update --label-add os=linux Linux-SwarmNode2
```

### <a name="deploying-services-to-a-mixed-os-swarm"></a>Развертывание служб в группе мелких объектов смешанных ОС
Метки для узлов группы мелких объектов упрощают развертывание служб в кластере; для этого достаточно использовать параметр `--constraint` для [`docker service create`](https://docs.docker.com/engine/reference/commandline/service_create/) команды:

```
# Deploy a service with swarm node constraint
C:\> docker service create --name=<SERVICENAME> --endpoint-mode dnsrr --network=<NETWORKNAME> --constraint node.labels.<LABELNAME>=<LABELVALUE> <CONTAINERIMAGE> [COMMAND] [ARGS…]
```

Если использовать метку и систему условных обозначений для значений метки из примера выше, набор команд для создания служб (одна команда для службы на основе Windows и одна команда для службы на основе Linux) будет иметь следующий вид:

```
# Example -- using the 'os' label and 'windows'/'linux' label values, service creation commands might look like these...

# A Windows service
C:\> docker service create --name=win_s1 --endpoint-mode dnsrr --network testoverlay --constraint 'node.labels.os==windows' microsoft/nanoserver:latest powershell -command { sleep 3600 }

# A Linux service
C:\> docker service create --name=linux_s1 --endpoint-mode dnsrr --network testoverlay --constraint 'node.labels.os==linux' redis
```

## <a name="limitations"></a>Ограничения
Сейчас режим мелких объектов в Windows имеет следующие ограничения.
- Шифрование плоскости данных не поддерживается (т. е. трафика между контейнерами с использованием параметра `--opt encrypted`).
- [Сетка маршрутизации](https://docs.docker.com/engine/swarm/ingress/) для узлов Windows DOCKER не поддерживается в windows Server 2016, но только начиная с windows Server 2019. Пользователи, которым требуется другая стратегия балансировки, могут установить внешнюю подсистему балансировки нагрузки (например, NGINX) и использовать [режим публикации порта](https://docs.docker.com/engine/reference/commandline/service_create/#/publish-service-ports-externally-to-the-swarm--p---publish) группы мелких объектов для предоставления доступа к портам узла контейнера, для которого требуется балансировка нагрузки. Подробнее об этом рассказывается ниже.

 >[!NOTE]
>Дополнительные сведения о настройке сетки маршрутизации DOCKER Swarm см. в этой [записи блога](https://docs.microsoft.com/en-us/virtualization/community/team-blog/2017/20170926-docker-s-routing-mesh-available-with-windows-server-version-1709) .

## <a name="publish-ports-for-service-endpoints"></a>Публикация портов для конечных точек службы
 Пользователи, которые хотят опубликовать порты для своих конечных точек служб, могут сделать это сегодня с помощью режима Publish-Port или функции [сетки маршрутизации](https://docs.docker.com/engine/swarm/ingress/) DOCKER Swarm. 

Чтобы опубликовать порты узла для каждой конечной точки задачи/контейнера, определяющей службу, используйте аргумент `--publish mode=host,target=<CONTAINERPORT>` для команды `docker service create`:

```
# Create a service for which tasks are exposed via host port
C:\ > docker service create --name=<SERVICENAME> --publish mode=host,target=<CONTAINERPORT> --endpoint-mode dnsrr --network=<NETWORKNAME> <CONTAINERIMAGE> [COMMAND] [ARGS…]
```

Например, следующая команда создает службу "s1", для которой каждая задача будет предоставлена через порт контейнера 80 и порт узла, выбранный случайным образом.

```
C:\ > docker service create --name=s1 --publish mode=host,target=80 --endpoint-mode dnsrr web_1 powershell -command {echo sleep; sleep 360000;}
```

После создания службы с помощью режима публикации портов службе можно отправить запрос на просмотр сопоставления портов для каждой задачи службы:

```
C:\ > docker service ps <SERVICENAME>
```
Приведенная выше команда возвращает подробные сведения о каждом экземпляра контейнера, запущенном для вашей службы (на всех узлах группы мелких объектов). Один столбец выходных данных, столбец Ports, будет включать сведения о портах для каждого узла формы \<ХОСТПОРТ\>->\<КОНТАИНЕРПОРТ\>/ТКП. Значения \<ХОСТПОРТ\> будут отличаться для каждого экземпляра контейнера, так как каждый контейнер публикуется на своем собственном порте узла.


## <a name="tips--insights"></a>Советы и полезные рекомендации 

#### <a name="existing-transparent-network-can-block-swarm-initializationoverlay-network-creation"></a>*Существующая Прозрачная сеть может блокировать создание Swarm инициализации или наложения сети.* 
В Windows внешний виртуальный коммутатор сетевых драйверов "overlay" и "transparent" должен быть привязан к (виртуальному) сетевому адаптеру узла. При создании сети наложения также создается новый коммутатор, который затем подключается к открытому сетевому адаптеру. В режиме прозрачного сетевого подключения также используется сетевой адаптер узла. В то же время любой сетевой адаптер одновременно можно привязать только к одному коммутатору. Если узел содержит только один сетевой адаптер, его можно подключить только к одному внешнему виртуальному коммутатору независимо от того, для какой сети предназначен этот виртуальный коммутатор — сети наложения или прозрачной сети. 

Следовательно, если узел контейнера содержит только один сетевой адаптер, может возникнуть проблема, при которой прозрачная сеть будет блокировать создание сети наложения (или наоборот), так как прозрачная сеть будет занимать виртуальный сетевой интерфейс только на узле.

Эту проблему можно устранить двумя способами.
- *Способ 1. Удаление существующей прозрачной сети.* Перед инициализацией группы мелких объектов убедитесь, что в узле контейнера отсутствует прозрачная сеть. Удалите прозрачные сети, чтобы обеспечить наличие свободного виртуального сетевого адаптера на узле. Этот адаптер будет использоваться для создания сети наложения.
- *Способ 2. Создание дополнительного (виртуального) сетевого адаптера на узле.* Вместо удаления прозрачной сети, расположенного на узле, можно создать дополнительный сетевой адаптер на вашем узле, который будет использоваться для создания сети наложения. Чтобы сделать это, создайте новый внешний сетевой адаптер (с помощью PowerShell или диспетчера Hyper-V). После создания нового интерфейса и при инициализации группы мелких объектов сетевая служба узлов (HNS) будет автоматически распознавать ее на вашем узле и использовать ее для привязки внешнего виртуального коммутатора в ходе создания сети наложения.



