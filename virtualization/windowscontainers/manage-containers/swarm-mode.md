---
title: Начало работы с режимом мелких объектов
description: Инициализация кластера мелких объектов, создание сети наложения и привязка службы к сети.
keywords: docker, контейнеры, управление группой мелких объектов
author: kallie-b
ms.date: 02/9/2017
ms.topic: article
ms.prod: windows-containers
ms.service: windows-containers
ms.assetid: 5ceb9626-7c48-4d42-81f8-9c936595ad85
ms.openlocfilehash: 088bc844790d94d30f6b4b05c5cd189392f47e66
ms.sourcegitcommit: cdf127747cfcb839a8abf50a173e628dcfee02db
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 08/07/2019
ms.locfileid: "9998281"
---
# <a name="getting-started-with-swarm-mode"></a>Начало работы с режимом мелких объектов 

## <a name="what-is-swarm-mode"></a>Что такое "режим мелких объектов"?
Режим мелких объектов — это функция Docker, которая предоставляет встроенные возможности управление контейнерами, в том числе кластеризацию узлов Docker и планирование рабочих нагрузок контейнера. Группа узлов Docker формирует кластер «мелких объектов», когда их модули Docker работают вместе в «режиме мелких объектов». Дополнительные сведения о режиме мелких объектов см. в [на основном сайте документации Docker](https://docs.docker.com/engine/swarm/).

## <a name="manager-nodes-and-worker-nodes"></a>Управляющие узлы и рабочие узлы
Группа мелких объектов состоит из двух типов узлов контейнера: *управляющих узлов* и *рабочих узлов*. Каждая группа мелких объектов инициализируется через управляющий узел, при этом все команды интерфейса командной строки Docker для мониторинга группы мелких объектов и управления ей должны выполняться с одного из управляющих узлов. Управляющие узлы можно рассматривать как «хранителей» состояния группы мелких объектов — вместе они формируют группу согласия, которая предоставляет сведения о состоянии служб, запущенных в группе мелких объектов. Их задача — убедиться, что фактическое состояние группы мелких объектов всегда соответствует желаемому состоянию, заданному разработчиком или администратором. 

>[!NOTE]
>Любой заданный Сварм может иметь несколько узлов диспетчера, но всегда должен иметь *хотя бы один*. 

Рабочие узлы управляются группой мелких объектов Docker посредством управляющих узлов. Чтобы присоединиться к группе мелких объектов, рабочий узел должен использовать «маркер присоединения», созданный с управляющего узла при инициализации группы мелких объектов. Рабочие узлы просто получают и выполняют задачи от управляющих узлов, поэтому для них не требуется информация о состоянии группы мелких объектов.

## <a name="swarm-mode-system-requirements"></a>Системные требования для режима мелких объектов

По крайней мере одна физическая или виртуальная система компьютера (для использования полной функциональности Сварм по крайней мере два узла) с помощью **обновления Windows 10 Creators** или **Windows Server 2016** *со всеми последними обновлениями: \ **, Настройка узел контейнера (Дополнительные сведения о том, как начать работу с контейнерами DOCKER в Windows 10, можно найти в разделе [контейнеры Windows в Windows 10](https://docs.microsoft.com/virtualization/windowscontainers/quick-start/quick-start-windows-10) или [контейнеры Windows на Windows Server](https://docs.microsoft.com/virtualization/windowscontainers/quick-start/quick-start-windows-server) ).

\***Примечание**. Для работы режима мелких объектов Docker в Windows Server 2016 требуется обновление [KB4015217](https://support.microsoft.com/help/4015217/windows-10-update-kb4015217).

**Модуль Docker 1.13.0 или более поздней версии**

Открытые порты: следующие порты должны быть доступны на каждом узле. В некоторых системах эти порты по умолчанию открыты.
- TCP-порт 2377 для управления кластером
- Порт TCP и UDP 7946 для связи между узлами
- Порт UDP 4789 для трафика сети наложения

## <a name="initializing-a-swarm-cluster"></a>Инициализация кластера мелких объектов

Чтобы инициализировать группу мелких объектов, выполните следующую команду на одном из узлов контейнера (заменив \<HOSTIPADDRESS\> на локальный IPv4-адрес хост-компьютера):

```
# Initialize a swarm 
C:\> docker swarm init --advertise-addr=<HOSTIPADDRESS> --listen-addr <HOSTIPADDRESS>:2377
```
После выполнения этой команды на узле контейнера модуль Docker на нем начинает работать в режиме мелких объектов в качестве управляющего узла.

## <a name="adding-nodes-to-a-swarm"></a>Добавление узлов в группу мелких объектов

Для использования функций режима Сварм и оверлея в сетевых режимах *не* требуется использовать несколько узлов. Все функции группы мелких объектов и сети наложения доступны при наличии одного узла, работающего в режиме мелких объектов (т. е. управляющего узла, переведенного в режим мелких объектов с помощью команды `docker swarm init`).

### <a name="adding-workers-to-a-swarm"></a>Добавление рабочих узлов в группу мелких объектов

После инициализации группы мелких объектов управляющим узлом в нее можно добавить рабочие узлы с помощью другой простой команды:

```
C:\> docker swarm join --token <WORKERJOINTOKEN> <MANAGERIPADDRESS>
```

Где \<MANAGERIPADDRESS\>— локальный IP-адрес управляющего узла группы мелких объектов, а \<WORKERJOINTOKEN\>— маркер присоединения рабочего узла из выходных данных команды `docker swarm init`, которая была выполнена на управляющем узле. Маркер присоединения также можно получить, выполнив одну из следующих команд на управляющем узле после инициализации группы мелких объектов:

```
# Get the full command required to join a worker node to the swarm
C:\> docker swarm join-token worker

# Get only the join-token needed to join a worker node to the swarm
C:\> docker swarm join-token worker -q
```

### <a name="adding-managers-to-a-swarm"></a>Добавление управляющих узлов в группу мелких объектов
Дополнительные управляющие узлы можно добавить в кластер мелких объектов с помощью следующей команды:

```
C:\> docker swarm join --token <MANAGERJOINTOKEN> <MANAGERIPADDRESS>
```

Опять же, \<MANAGERIPADDRESS\>— это локальный IP-адрес управляющего группы мелких объектов. Маркер присоединения \<MANAGERJOINTOKEN\>— это маркер присоединения *управляющего узла* для группы мелких объектов, который можно получить, выполнив одну из следующих команд на существующем управляющем узле:

```
# Get the full command required to join a **manager** node to the swarm
C:\> docker swarm join-token manager

# Get only the join-token needed to join a **manager** node to the swarm
C:\> docker swarm join-token manager -q
```

## <a name="creating-an-overlay-network"></a>Создание сети наложения

После настройки кластера мелких объектов можно создать сети наложения. Для этого выполните следующую команду на управляющем узле группы мелких объектов:

```
# Create an overlay network 
C:\> docker network create --driver=overlay <NETWORKNAME>
```

Здесь \<NETWORKNAME\>— это имя вашей сети.

## <a name="deploying-services-to-a-swarm"></a>Развертывание служб в группе мелких объектов
После создания сети наложения можно создать службы и привязать их к сети. Служба создается с помощью следующего синтаксиса:

```
# Deploy a service to the swarm
C:\> docker service create --name=<SERVICENAME> --endpoint-mode dnsrr --network=<NETWORKNAME> <CONTAINERIMAGE> [COMMAND] [ARGS…]
```

Здесь \<SERVICENAME\>— это имя службы, которое будет использоваться для ссылки на службу при обнаружении служб (для чего применяется собственный DNS-сервер Docker). \<NETWORKNAME\>— это имя сети, к которой следует подключить эту службу (например, "myOverlayNet"). \<CONTAINERIMAGE\>— это имя образа контейнера, в котором будет определена служба.

>[!NOTE]
>Вторым аргументом этой команды `--endpoint-mode dnsrr`является указание обработчика DOCKER, который будет использоваться политикой циклического передвижения DNS для баланса сетевого трафика между конечными точками контейнера служб. Сейчас циклический перебор DNS — единственная стратегия балансировки, поддерживаемая в Windows. [Сетка маршрутизации](https://docs.docker.com/engine/swarm/ingress/) для узлов Docker в Windows не поддерживается, но эта возможность будет добавлена в ближайшее время. Пользователи, которым требуется другая стратегия балансировки, могут установить внешнюю подсистему балансировки нагрузки (например, NGINX) и использовать [режим публикации порта](https://docs.docker.com/engine/reference/commandline/service_create/#/publish-service-ports-externally-to-the-swarm--p---publish) группы мелких объектов для предоставления доступа к портам узла контейнера, для которого требуется балансировка нагрузки.

## <a name="scaling-a-service"></a>Масштабирование службы
После развертывания службы в кластере мелких объектов экземпляры контейнера, из которых она состоит, развертываются в кластере. По умолчанию количество экземпляров контейнера, поддерживающих службу (число «реплик» или «задач» для службы) равно одному. Тем не менее службу можно создать с несколькими задачами, используя параметр `--replicas` для команды `docker service create` или масштабируя службу после ее создания.

Масштабируемость службы— важное преимущество Docker Swarm, и им можно воспользоваться с помощью одной команды Docker:

```
C:\> docker service scale <SERVICENAME>=<REPLICAS>
```

Здесь \<SERVICENAME\>— имя масштабируемой службы, а \<REPLICAS\>— это число задач или экземпляров контейнера, до которого масштабируется служба.


## <a name="viewing-the-swarm-state"></a>Просмотр состояния группы мелких объектов

Существует несколько полезных команд для просмотра состояния группы мелких объектов и служб, запущенных в ней.

### <a name="list-swarm-nodes"></a>Перечисление узлов группы мелких объектов
Выполните следующую команду, чтобы просмотреть список узлов, присоединенных к группе мелких объектов, включая сведения о состоянии каждого узла. Эту команда необходимо выполнить на **управляющем узле**.

```
C:\> docker node ls
```

В выходных данных этой команды можно заметить, что один из узлов помечен звездочкой (*). Этот символ просто обозначает текущий узел, на котором была выполнена команда `docker node ls`.

### <a name="list-networks"></a>Перечисление сетей
Выполните следующую команду, чтобы просмотреть список сетей на заданном узле. Для просмотра сетей наложения эту команда следует выполнить на **управляющем узле**, работающем в режиме мелких объектов.

```
C:\> docker network ls
```

### <a name="list-services"></a>Перечисление служб
Выполните следующую команду, чтобы просмотреть список служб, запущенных в группе мелких объектов, а также сведения об их состоянии.

```
C:\> docker service ls
```

### <a name="list-the-container-instances-that-define-a-service"></a>Перечисление экземпляров контейнера, которые определяют службу
Выполните следующую команду, чтобы просмотреть сведения об экземплярах контейнера, запущенных для указанной службы. В выходные данные этой команды включены идентификаторы и узлы, на которых работает каждый контейнер, а также информация о состоянии контейнеров.  

```
C:\> docker service ps <SERVICENAME>
```
## <a name="linuxwindows-mixed-os-clusters"></a>Кластеры со смешанными ОС Linux и Windows

Недавно член нашей команды опубликовал короткое руководство из трех частей, рассказывающее о том, как с помощью режима мелких объектов Docker настроить приложение для смешанных ОС Windows + Linux. Если вы только начинаете работать с режимом мелких объектов Docker или использовать его для запуска приложений для смешанных ОС, это руководство будет очень полезным. Ознакомьтесь с ним.
- [Использование режима мелких объектов Docker для запуска контейнерного приложения для Windows + Linux (часть 1 из 3)](https://www.youtube.com/watch?v=ZfMV5JmkWCY&t=170s)
- [Использование режима мелких объектов Docker для запуска контейнерного приложения для Windows + Linux (часть 2 из 3)](https://www.youtube.com/watch?v=VbzwKbcC_Mg&t=406s)
- [Использование режима мелких объектов Docker для запуска контейнерного приложения для Windows + Linux (часть 3 из 3)](https://www.youtube.com/watch?v=I9oDD78E_1E&t=354s)

### <a name="initializing-a-linuxwindows-mixed-os-cluster"></a>Инициализация кластера смешанных ОС Linux + Windows
Инициализировать кластер мелких объектов смешанных ОС не составит труда, если правила вашего брандмауэра корректно настроены и у ваших узлов есть доступ друг к другу. Для добавления узла Linux в группу мелких объектов потребуется лишь стандартная команда `docker swarm join`:
```
C:\> docker swarm join --token <JOINTOKEN> <MANAGERIPADDRESS>
```
Из узла Linux группу мелких объектов можно также инициализировать с помощью той же команды, которая используется для инициализации группы мелких объектов из узла Windows:
```
# Initialize a swarm 
C:\> docker swarm init --advertise-addr=<HOSTIPADDRESS> --listen-addr <HOSTIPADDRESS>:2377
```

### <a name="adding-labels-to-swarm-nodes"></a>Добавление меток в узлы группы мелких объектов
Чтобы запустить службу Docker в кластере мелких объектов смешанных ОС, должен быть способ определить, какие узлы группы мелких объектов работают под управлением ОС, для которой предназначена эта служба, а какие нет. [Метки объектов Docker](https://docs.docker.com/engine/userguide/labels-custom-metadata/)— это удобный способ помечать узлы таким образом, чтобы службы можно было создавать и настраивать для выполнения только на тех узлах, которые соответствуют их ОС. 

>[!NOTE]
>[Метки объектов](https://docs.docker.com/engine/userguide/labels-custom-metadata/) -закрепления можно использовать для применения метаданных к различным объектам Dock (включая изображения-контейнеры, контейнеры, тома и сети) и в различных целях (например, метки могут использоваться для разделения одноранговых и серверных компонентов приложение, разрешающее интерфейсные высококонечные службы, сечедулед только на узлы с подписями и серверные миркосервицес, которые будут планироваться только на узлах с подписями на стороне сервера. В этом случае метки используются для того, чтобы узлы ОС Windows можно было отличить от узлов ОС Linux.

Чтобы пометить существующие узлы группы мелких объектов, используйте следующий синтаксис:

```
C:\> docker node update --label-add <LABELNAME>=<LABELVALUE> <NODENAME>
```

Здесь `<LABELNAME>`— имя создаваемой метки. В этом примере мы проводим различие между узлами по их ОС, поэтому логичным именем для этой метки может быть "os". `<LABELVALUE>` — это значение метки. В этом случае можно использовать значения "windows" и "linux". Разумеется, вы можете присваивать собственные имена и значения меткам, однако они не должны противоречить друг другу. `<NODENAME>` — имя помечаемого узла; имена узлов можно выводить с помощью команды `docker node ls`. 

**Например**, если у вас есть четыре узла группы мелких объектов в кластере (то есть два узла Windows и два узла Linux), команды для обновления метки могут иметь следующий вид:

```
# Example -- labeling 2 Windows nodes and 2 Linux nodes in a cluster...
C:\> docker node update --label-add os=windows Windows-SwarmMaster
C:\> docker node update --label-add os=windows Windows-SwarmWorker1
C:\> docker node update --label-add os=linux Linux-SwarmNode1
C:\> docker node update --label-add os=linux Linux-SwarmNode2
```

### <a name="deploying-services-to-a-mixed-os-swarm"></a>Развертывание служб в группе мелких объектов смешанных ОС
Метки для узлов группы мелких объектов упрощают развертывание служб в кластере; для этого достаточно использовать параметр `--constraint` для [`docker service create`](https://docs.docker.com/engine/reference/commandline/service_create/) команды:

```
# Deploy a service with swarm node constraint
C:\> docker service create --name=<SERVICENAME> --endpoint-mode dnsrr --network=<NETWORKNAME> --constraint node.labels.<LABELNAME>=<LABELVALUE> <CONTAINERIMAGE> [COMMAND] [ARGS…]
```

Если использовать метку и систему условных обозначений для значений метки из примера выше, набор команд для создания служб (одна команда для службы на основе Windows и одна команда для службы на основе Linux) будет иметь следующий вид:

```
# Example -- using the 'os' label and 'windows'/'linux' label values, service creation commands might look like these...

# A Windows service
C:\> docker service create --name=win_s1 --endpoint-mode dnsrr --network testoverlay --constraint 'node.labels.os==windows' microsoft/nanoserver:latest powershell -command { sleep 3600 }

# A Linux service
C:\> docker service create --name=linux_s1 --endpoint-mode dnsrr --network testoverlay --constraint 'node.labels.os==linux' redis
```

## <a name="limitations"></a>Ограничения
Сейчас режим мелких объектов в Windows имеет следующие ограничения.
- Шифрование плоскости данных не поддерживается (т. е. трафика между контейнерами с использованием параметра `--opt encrypted`).
- [Сетка маршрутизации](https://docs.docker.com/engine/swarm/ingress/) для узлов Docker в Windows пока не поддерживается, но эта возможность будет добавлена в ближайшее время. Пользователи, которым требуется другая стратегия балансировки, могут установить внешнюю подсистему балансировки нагрузки (например, NGINX) и использовать [режим публикации порта](https://docs.docker.com/engine/reference/commandline/service_create/#/publish-service-ports-externally-to-the-swarm--p---publish) группы мелких объектов для предоставления доступа к портам узла контейнера, для которого требуется балансировка нагрузки. Подробнее об этом рассказывается ниже.

## <a name="publish-ports-for-service-endpoints"></a>Публикация портов для конечных точек службы
[Сетка маршрутизации](https://docs.docker.com/engine/swarm/ingress/) режима мелких объектов Docker еще не поддерживается в Windows, однако пользователи, которым требуется опубликовать порты для конечных точек службы, могут сделать это с помощью режима публикации портов. 

Чтобы опубликовать порты узла для каждой конечной точки задачи/контейнера, определяющей службу, используйте аргумент `--publish mode=host,target=<CONTAINERPORT>` для команды `docker service create`:

```
# Create a service for which tasks are exposed via host port
C:\ > docker service create --name=<SERVICENAME> --publish mode=host,target=<CONTAINERPORT> --endpoint-mode dnsrr --network=<NETWORKNAME> <CONTAINERIMAGE> [COMMAND] [ARGS…]
```

Например, следующая команда создает службу "s1", для которой каждая задача будет предоставлена через порт контейнера 80 и порт узла, выбранный случайным образом.

```
C:\ > docker service create --name=s1 --publish mode=host,target=80 --endpoint-mode dnsrr web_1 powershell -command {echo sleep; sleep 360000;}
```

После создания службы с помощью режима публикации портов службе можно отправить запрос на просмотр сопоставления портов для каждой задачи службы:

```
C:\ > docker service ps <SERVICENAME>
```
Приведенная выше команда возвращает подробные сведения о каждом экземпляра контейнера, запущенном для вашей службы (на всех узлах группы мелких объектов). В одном из столбцов данных вывода (столбец "ports") будут содержаться сведения о портах для каждого узла в виде \<HOSTPORT\>->\<CONTAINERPORT\>/tcp. Значения \<HOSTPORT\> будут разными для каждого экземпляра контейнера, так как публикация каждого контейнера выполняется на его собственном порте узла.


## <a name="tips--insights"></a>Советы и полезные рекомендации 

#### *<a name="existing-transparent-network-can-block-swarm-initializationoverlay-network-creation"></a>Существующая прозрачная сеть может блокировать инициализацию группы мелких объектов или создание сети наложения* 
В Windows внешний виртуальный коммутатор сетевых драйверов "overlay" и "transparent" должен быть привязан к (виртуальному) сетевому адаптеру узла. При создании сети наложения также создается новый коммутатор, который затем подключается к открытому сетевому адаптеру. В режиме прозрачного сетевого подключения также используется сетевой адаптер узла. В то же время любой сетевой адаптер одновременно можно привязать только к одному коммутатору. Если узел содержит только один сетевой адаптер, его можно подключить только к одному внешнему виртуальному коммутатору независимо от того, для какой сети предназначен этот виртуальный коммутатор— сети наложения или прозрачной сети. 

Следовательно, если узел контейнера содержит только один сетевой адаптер, может возникнуть проблема, при которой прозрачная сеть будет блокировать создание сети наложения (или наоборот), так как прозрачная сеть будет занимать виртуальный сетевой интерфейс только на узле.

Эту проблему можно устранить двумя способами.
- *Способ 1. Удаление существующей прозрачной сети.* Перед инициализацией группы мелких объектов убедитесь, что в узле контейнера отсутствует прозрачная сеть. Удалите прозрачные сети, чтобы обеспечить наличие свободного виртуального сетевого адаптера на узле. Этот адаптер будет использоваться для создания сети наложения.
- *Способ 2. Создание дополнительного (виртуального) сетевого адаптера на узле.* Вместо удаления прозрачной сети, расположенного на узле, можно создать дополнительный сетевой адаптер на вашем узле, который будет использоваться для создания сети наложения. Чтобы сделать это, создайте новый внешний сетевой адаптер (с помощью PowerShell или диспетчера Hyper-V). После создания нового интерфейса и при инициализации группы мелких объектов сетевая служба узлов (HNS) будет автоматически распознавать ее на вашем узле и использовать ее для привязки внешнего виртуального коммутатора в ходе создания сети наложения.



