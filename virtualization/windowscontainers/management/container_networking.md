---
title: "Сетевые подключения контейнеров Windows"
description: "Настройка сетевого взаимодействия для контейнеров Windows."
keywords: "docker, контейнеры"
author: jmesser81
manager: timlt
ms.date: 08/22/2016
ms.topic: article
ms.prod: windows-containers
ms.service: windows-containers
ms.assetid: 538871ba-d02e-47d3-a3bf-25cda4a40965
translationtype: Human Translation
ms.sourcegitcommit: f489d3e6f98fd77739a2016813506be6962b34d1
ms.openlocfilehash: 499788666f306494c894b2e82f65ab68c9fc295a

---

# Сетевые подключения контейнеров

В отношении сетевых подключений контейнеры Windows функционируют аналогично виртуальным машинам. В каждом контейнере имеется виртуальный сетевой адаптер (vNIC), подключенный к виртуальному коммутатору (vSwitch), по которому перенаправляется входящий и исходящий трафик. Чтобы обеспечить изоляцию между контейнерами на одном узле, для каждого контейнера Windows Server и Hyper-V создается секция сети, куда устанавливается сетевой адаптер этого контейнера. Для подключения к виртуальному коммутатору контейнеры Windows Server используют виртуальный сетевой адаптер узла. Для подключения к виртуальному коммутатору контейнеры Hyper-V используют сетевой адаптер синтетической виртуальной машины (не предоставляется служебной виртуальной машине).

Контейнеры Windows поддерживают четыре разных сетевых драйвера или режима: *nat*, *transparent*, *l2bridge* и *l2tunnel*. В зависимости от физической сетевой инфраструктуры и от того, должна ли сеть быть одно- или многоузловой, необходимо выбрать сетевой режим, который наилучшим образом соответствует вашим потребностям. 

По умолчанию при первом запуске службы Docker подсистема Docker создает сеть NAT. По умолчанию созданный внутренний префикс IP-адреса имеет значение 172.16.0.0/12. Конечные точки контейнера будут автоматически подключены к этой сети по умолчанию, и им будут назначены IP-адреса из внутреннего префикса.

> Примечание. Если IP-адрес узла контейнера находится в том же префиксе, необходимо будет изменить внутренний префикс IP-адреса NAT, как описано ниже.

Дополнительные сети, использующие другой драйвер (например, transparent или l2bridge), можно создавать в том же узле контейнера. В следующей таблице показано, как сетевое подключение предоставляется для внутренних (контейнер — контейнер) и внешних подключений каждого режима.

- **Преобразование сетевых адресов** — каждый контейнер получает IP-адрес из внутреннего частного префикса IP-адреса (например, 172.16.0.0/12). Перенаправление и сопоставление портов из узла контейнера в конечные точки контейнера поддерживается

- **Прозрачный режим** — каждая конечная точка контейнера подключена напрямую к физической сети. IP-адреса из физической сети могут назначаться статически или динамически с помощью внешнего DHCP-сервера.

- **Режим моста L2** — каждая конечная точка контейнера будет находиться в той же IP-подсети, что и узел контейнера. IP-адреса должны назначаться статически из того же префикса, что и узел контейнера. Все конечные точки контейнера на узле будут иметь одинаковый MAC-адрес из-за преобразования адресов второго уровня.

- **Туннельный режим L2** - _ — этот режим должен использоваться только в Microsoft Cloud Stack_.

> Сведения о подключении конечных точек контейнера к дополнительной виртуальной сети со стеком Microsoft SDN см. в статье [Подключение контейнеров к виртуальной сети](https://technet.microsoft.com/en-us/windows-server-docs/networking/sdn/manage/connect-container-endpoints-to-a-tenant-virtual-network).

## С одним узлом

|  | Контейнер — контейнер | Контейнер — внешние |
| :---: | :---------------     |  :---                |
| nat | Подключение с мостом через виртуальный коммутатор Hyper-V | маршрутизация через WinNAT с применением преобразования адресов | 
| transparent | Подключение с мостом через виртуальный коммутатор Hyper-V | прямой доступ к физической сети | 
| l2bridge | Подключение с мостом через виртуальный коммутатор Hyper-V|  доступ к физической сети с преобразованием MAC-адресов|  



## С несколькими узлами

|  | Контейнер — контейнер | Контейнер — внешние |
| :---: | :----       | :---------- |
| nat | должен ссылаться на IP-адрес и порт узла внешнего контейнера; маршрутизация через WinNAT с применением преобразования адресов | должен ссылаться на внешний узел; маршрутизация через WinNAT с применением преобразования адресов | 
| transparent | должен ссылаться на конечную точку IP-адреса контейнера напрямую | прямой доступ к физической сети | 
| l2bridge | должен ссылаться на конечную точку IP-адреса контейнера напрямую| доступ к физической сети с преобразованием MAC-адресов| 


## Создание сети 

### (По умолчанию.) Сеть NAT

Подсистема Docker Windows создает сеть NAT по умолчанию (в Docker — "nat") с префиксом IP-адреса 172.16.0.0/12. Если нужно создать сеть NAT с конкретным префиксом IP-адреса, можно выполнить одно из двух следующих действий, изменив параметры в файле конфигурации Docker daemon.json (расположен в папке "C:\ProgramData\Docker\config\daemon.json"; создайте его, если он еще не существует).
 1. Использовать параметр _"fixed-cidr": "< IP-префикс > / Mask"_, который создает сеть NAT по умолчанию с указанными префиксом IP-адреса и маской.
 2. Использовать параметр _"bridge": "none"_, который не создает сеть по умолчанию; пользователь может создать пользовательскую сеть с любым драйвером с помощью команды *docker network create -d<driver>*.

Перед использованием любого из этих вариантов настройки необходимо предварительно остановить службу Docker и удалить все существующие сети NAT.

```none
PS C:\> Stop-Service docker
PS C:\> Get-ContainerNetwork | Remove-ContainerNetwork

...Edit the daemon.json file...

PS C:\> Start-Service docker
```

При добавлении параметра "fixed-cidr" в файл daemon.json подсистема Docker создаст пользовательскую сеть NAT с указанным IP-префиксом и маской. При использовании же параметра "bridge:none" сеть потребуется создать вручную.

```none
# Create a user-defined NAT network
C:\> docker network create -d nat --subnet=192.168.1.0/24 --gateway=192.168.1.1 MyNatNetwork
```

По умолчанию конечные точки контейнера будут подключены к сети NAT, выбранной по умолчанию. Если сеть NAT не создана (так как в файле daemon.json указан параметр "bridge:none") или требуется доступ к другой пользовательской сети, можно указать параметр *--network* для команды "run" Docker.

```none
# Connect new container to the MyNatNetwork
C:\> docker run -it --network=MyNatNetwork <image> <cmd>
```

#### Сопоставление портов

Чтобы получить доступ к приложениям, выполняющимся внутри контейнера, подключенного к сети NAT, между узлом контейнера и конечной точкой контейнера должны быть созданы сопоставления портов. Эти сопоставления должны быть указаны в момент создания контейнера или тогда, когда контейнер остановлен.

```none
# Creates a static mapping between port TCP:80 of the container host and TCP:80 of the container
C:\> docker run -it -p 80:80 <image> <cmd>

# Creates a static mapping between port 8082 of the container host and port 80 of the container.
C:\> docker run -it -p 8082:80 windowsservercore cmd
```

Динамическое сопоставление портов можно выполнять с помощью параметра -p или команды EXPOSE с параметром -P в Dockerfile. Если ни один из них не указан, на узле контейнера выбирается случайный временный порт, который можно проверить командой "docker ps".

```none
C:\> docker run -itd -p 80 windowsservercore cmd

# Network services running on port TCP:80 in this container can be accessed externally on port TCP:14824
C:\> docker ps
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS                   NAMES
bbf72109b1fc        windowsservercore   "cmd"               6 seconds ago       Up 2 seconds        *0.0.0.0:14824->80/tcp*   drunk_stonebraker

# Container image specified EXPOSE 80 in Dockerfile - publish this port mapping
C:\> docker network 
```
> Начиная с WS2016 TP5 и сборок Windows для участников программы предварительной оценки (версий старше 14300), правило брандмауэра автоматически создается для всех сопоставлений портов NAT. Оно является глобальным по отношению к узлу контейнера и не ограничивается конкретным контейнером конечной точки или сетевым адаптером.

Реализация Windows NAT (WinNAT) имеет несколько недостатков, которые рассматриваются в этой публикации блога [WinNAT capabilities and limitations](https://blogs.technet.microsoft.com/virtualization/2016/05/25/windows-nat-winnat-capabilities-and-limitations/) (Возможности и ограничения WinNAT). 
 1. На одном узле контейнера поддерживается только один внутренний IP-префикс NAT, поэтому несколько сетей NAT задаются при помощи секционирования префикса (см. раздел "Несколько сетей NAT" этого документа).
 2. Конечные точки контейнера доступны только из узла контейнера, использующего внутренние IP-адреса и порты контейнера (эту информацию можно найти командой "docker network inspect <CONTAINER ID>").

Дополнительные сети можно создавать с помощью других драйверов. 

> Имена всех сетевых драйверов Docker состоят из строчных букв.

### Прозрачная сеть

Для использования режима прозрачного сетевого взаимодействия создайте сеть контейнера с именем драйвера "transparent". 

```none
C:\> docker network create -d transparent MyTransparentNetwork
```
> Примечание. Если при создании прозрачной сети возникнет ошибка, возможно, в системе есть внешний виртуальный коммутатор, который не был автоматически обнаружен Docker и не позволяет привязать прозрачную сеть к внешнему сетевому адаптеру узла контейнера. Дополнительные сведения см. в подразделе ниже, "Существующий виртуальный коммутатор блокирует создание прозрачной сети", раздела "Особенности и рекомендации".

Если узел контейнера виртуализирован и вы хотите использовать DHCP для назначения IP-адресов, необходимо включить MACAddressSpoofing на сетевом адаптере виртуальных машин. В противном случае узел Hyper-V будет блокировать сетевой трафик от контейнеров к виртуальной машине с несколькими MAC-адресами.

```none
PS C:\> Get-VMNetworkAdapter -VMName ContainerHostVM | Set-VMNetworkAdapter -MacAddressSpoofing On
```

> Чтобы создать несколько прозрачных сетей, необходимо указать, к какому сетевому адаптеру (виртуальному) должен быть привязан внешний виртуальный коммутатор Hyper-V (создаваемый автоматически).

Чтобы привязать сеть, подключенную через виртуальный коммутатор Hyper-V, к конкретному сетевому интерфейсу, используйте параметр *-o com.docker.network.windowsshim.interface=<Interface>*.

```none
# Create a transparent network which is attached to the "Ethernet 2" network interface
C:\> docker network create -d transparent -o com.docker.network.windowsshim.interface="Ethernet 2" TransparentNet2
```

Значением *com.docker.network.windowsshim.interface* является *Name* (Имя) адаптера из: 
```none
Get-NetAdapter
```

IP-адреса для конечных точек контейнера, подключенных к прозрачной сети, можно назначить статически или динамически с внешнего DHCP-сервера.

При статическом назначении IP-адресов необходимо сначала убедиться, что при создании сети указаны параметры *--subnet* и *--gateway*. IP-адрес подсети и шлюза должен совпадать с IP-адресом, указанным в параметрах сети для контейнера узла, т. е. физической сети.

```none
# Create a transparent network corresponding to the physical network with IP prefix 10.123.174.0/23
C:\> docker network create -d transparent --subnet=10.123.174.0/23 --gateway=10.123.174.1 TransparentNet3
```
Укажите IP-адрес для команды "run" Docker с помощью параметра *--ip*.

```none
C:\> docker run -it --network=TransparentNet3 --ip 10.123.174.105 <image> <cmd>
```

> Убедитесь, что этот IP-адрес не назначен другому сетевому устройству в физической сети.

Поскольку конечные точки контейнера имеют прямой доступ к физической сети, нет необходимости указывать сопоставления портов.

### Мост второго уровня 

Для использования сетевого режима моста L2 создайте сеть контейнера с именем драйвера "l2bridge". Необходимо опять же указать подсеть и шлюз, соответствующие физической сети.

```none
C:\> docker network create -d l2bridge --subnet=192.168.1.0/24 --gateway=192.168.1.1 MyBridgeNetwork
```

В сетях l2bridge поддерживается только статическое назначение IP-адресов. 

> При использовании сети l2bridge в структуре SDN поддерживается только динамическое назначение IP-адресов. Дополнительные сведения см. в статье [Подключение контейнеров к виртуальной сети](https://technet.microsoft.com/en-us/windows-server-docs/networking/sdn/manage/connect-container-endpoints-to-a-tenant-virtual-network).

## Другие операции и конфигурации

### Перечисление доступных сетей

```none
# list container networks
C:\> docker network ls

NETWORK ID          NAME                DRIVER              SCOPE
0a297065f06a        nat                 nat                 local
d42516aa0250        none                null                local
```

### Удаление сети

Используйте команду `docker network rm`, чтобы удалить сеть контейнера.

```none
C:\> docker network rm "<network name>"
```

Это приведет к очистке виртуальных коммутаторов Hyper-V, используемых сетью контейнера, а также всех созданных преобразований сетевых адресов (экземпляров WinNAT — NetNat).

### Анализ сети 

Чтобы узнать, какие контейнеры подключены к определенной сети и какие IP-адреса связаны с этими конечными точками контейнера, можно выполнить приведенную ниже команду.

```none
C:\> docker network inspect <network name>
```

### Несколько сетей контейнера
 В Windows сейчас поддерживается только одна сеть NAT (хотя с помощью ожидающего [запроса на включение внесенных изменений](https://github.com/docker/docker/pull/25097) можно обойти это ограничение). 

На одном узле контейнера можно создать несколько сетей контейнера со следующими оговорками:

* Каждая из нескольких сетей, использующих внешний коммутатор vSwitch для взаимодействия (например, прозрачный режим, мост L2, прозрачный режим L2), должна использовать свой собственный сетевой адаптер.
* Сейчас для создания нескольких сетей NAT на одном узле контейнера используется секционирование внутреннего префикса существующей сети NAT. Дополнительные сведения см. в разделе ниже: "Несколько сетей NAT".

### Несколько сетей NAT
Несколько сетей NAT можно определить в одном узле контейнера путем секционирования внутреннего префикса сети NAT узла. 

Разделы для новых сетей NAT должны быть созданы в префиксе более крупной внутренней сети NAT. Чтобы найти префикс, запустите следующую команду из PowerShell со ссылкой на поле InternalIPInterfaceAddressPrefix.

```none
PS C:\> get-netnat
```

Например, внутренний префикс сети NAT узла может быть 172.16.0.0/12. В этом случае Docker можно использовать для создания дополнительных сетей NAT, *если они находятся в пределах префикса 172.16.0.0/12.* Например, можно создать две сети NAT с IP-префиксами 172.16.0.0/16 (шлюз, 172.16.0.1) и 172.17.0.0/16 (шлюз, 172.17.0.1). 

```none
C:\> docker network create -d nat --subnet=172.16.0.0/16 --gateway=172.16.0.1 CustomNat1
C:\> docker network create -d nat --subnet=172.17.0.0/16 --gateway=172.17.0.1 CustomNat2
```

Созданные сети можно перечислить при помощи следующих элементов.
```none
C:\> docker network ls
```


### Выбор сетей

При создании контейнера Windows можно указать сеть, к которой будет подключен сетевой адаптер контейнера. Если сеть не указана, используется сеть NAT по умолчанию.

Чтобы подключить контейнер к сети NAT, отличной от применяемой по умолчанию, используйте параметр --network для команды "run" Docker.

```none
C:\> docker run -it --network=MyTransparentNet windowsservercore cmd
```

### Статический IP-адрес

```none
C:\> docker run -it --network=MyTransparentNet --ip=10.80.123.32 windowsservercore cmd
```

Назначение статических IP-адресов выполняется непосредственно на сетевом адаптере контейнера и должно осуществляться, только когда контейнер находится в ОСТАНОВЛЕННОМ состоянии. "Горячее добавление" сетевых адаптеров контейнера или изменений в сетевой стек не поддерживается во время выполнения контейнера.

## Docker Compose и Service Discovery

> Практический пример использования Docker Compose и Service Discovery для определения горизонтально масштабируемых приложений, состоящих из нескольких служб, см. в [этой публикации](https://blogs.technet.microsoft.com/virtualization/2016/10/18/use-docker-compose-and-service-discovery-on-windows-to-scale-out-your-multi-service-container-application/) [блога виртуализации](https://blogs.technet.microsoft.com/virtualization/).

### Docker Compose

[Docker Compose](https://docs.docker.com/compose/overview/) можно использовать для определения и настройки сетей контейнера вместе с контейнерами и службами, которые будут использовать эти сети. Ключ "networks" Compose используется как ключ верхнего уровня при определении сетей, к которым будут подключены контейнеры. Например, в синтаксисе ниже определена существующая сеть NAT, создаваемая Docker в качестве сети по умолчанию для всех контейнеров и служб, определенных в указанном файле Compose.

```none
networks:
 default:
  external:
   name: "nat"
```

Аналогично можно использовать следующий синтаксис для определения пользовательской сети NAT.

> Примечание. Пользовательская сеть NAT, указанная в примере ниже, определена как раздел внутреннего префикса существующей сети NAT узла контейнера. Дополнительные сведения см. в разделе выше, "Несколько сетей NAT".

```none
networks:
  default:
    driver: nat
    ipam:
      driver: default
      config:
      - subnet: 172.17.0.0/16
```

Более подробная информация об определении и настройке сетей контейнера при помощи Docker Compose см. в [справочнике по файлам Compose](https://docs.docker.com/compose/compose-file/).

### Service Discovery
Встроенное в Docker Service Discovery (обнаружение служб) обрабатывает регистрацию служб и сопоставление имен с IP-адресами (DNS) для контейнеров и служб. При использовании обнаружения служб все конечные точки контейнера могут обнаруживать друг друга по имени (по имени контейнера или имени службы). Это особенно важно при горизонтальном масштабировании, где для определения одной службы используется несколько конечных точек контейнера. В таких случаях обнаружение служб позволяет считать службу единой сущностью независимо от того, сколько контейнеров в ней запущено. Для служб с несколькими контейнерами входящий сетевой трафик управляется при помощи циклического перебора, по которому балансировка нагрузки DNS используется для равномерного распределения трафика по всем экземплярам контейнера, реализующим конкретную службу.

## Особенности и рекомендации

### Брандмауэр

Для узла контейнера требуется создать определенные правила брандмауэра, чтобы разрешить ICMP (проверка связи) и DHCP. Протоколы ICMP и DHCP нужны контейнерам Windows Server для проверки связи между двумя контейнерами на одном узле, а также для получения динамически назначаемых IP-адресов через DHCP. В TP5 эти правила создаются с помощью сценария Install-ContainerHost.ps1. В более поздних версиях они будут создаваться автоматически. Все правила брандмауэра, соответствующие правилам перенаправления портов NAT, будут создаваться автоматически и очищаться после остановки контейнера.

### Существующий виртуальный коммутатор блокирует создание прозрачной сети

При создании прозрачной сети Docker создает внешний виртуальный коммутатор для сети, а затем пытается привязать коммутатор к (внешнему) сетевому адаптеру. Адаптер может быть сетевым адаптером VM или физическим сетевым адаптером. Если виртуальный коммутатор уже создан на узле контейнера *и видим Docker*, подсистема Windows Docker будет использовать этот коммутатор, а не создавать новый. Но если виртуальный коммутатор создан внешними средствами (например, создан на узле контейнера при помощи диспетчера Hyper-V или PowerShell) и еще не виден Docker, подсистема Windows Docker попытается создать виртуальный коммутатор, после чего она не сможет подключить новый коммутатор к внешнему адаптеру сети узла контейнера (так как сетевой адаптер уже будет подключен к коммутатору, созданному внешними средствами).

Например, эта проблема может возникнуть, если вы сначала создали виртуальный коммутатор на узле, когда служба Docker была запущена, после чего попытались создать прозрачную сеть. В этом случае Docker не распознает созданный коммутатор и создаст новый виртуальный коммутатор для прозрачной сети.

Существует три подхода к решению этой проблемы.

* Вы можете удалить виртуальный коммутатор, созданный внешними средствами, что позволит Docker создать новый виртуальный коммутатор и подключить его к сетевому адаптеру узла без проблем. Перед тем как выбрать этот подход убедитесь, что виртуальный коммутатор, созданный внешними средствами, не используется другими службами (например, Hyper-V).
* Кроме того, если вы захотите использовать внешний виртуальный коммутатор, созданный внешними средствами, перезапустите службы Docker и HNS, чтобы *коммутатор стал видим Docker.*
```none
PS C:\> restart-service hns
PS C:\> restart-service docker
```
* Другой вариант — использовать "-o com.docker.network.windowsshim.interface1", чтобы привязать внешний виртуальный коммутатор прозрачной сети к определенному сетевому адаптеру, который еще не используется в узле контейнера (например, сетевой адаптер, отличный от используемого виртуальным коммутатором, созданным внешними средствами). Параметр "-o" описан выше, в разделе [Прозрачная сеть](https://msdn.microsoft.com/en-us/virtualization/windowscontainers/management/container_networking#transparent-network) этого документа.

### Неподдерживаемые функции

Сейчас через Docker CLI нельзя выполнить приведенные ниже сетевые функции:
 * Драйвер наложенной сети по умолчанию.
 * Связывание контейнера (например, --link)

Сейчас в Windows Docker не поддерживаются следующие параметры сети:
 * --add-host
 * --dns
 * --dns-opt
 * --dns-search
 * -h, --hostname
 * --net-alias
 * --aux-address
 * --internal
 * --ip-range



<!--HONumber=Oct16_HO4-->


